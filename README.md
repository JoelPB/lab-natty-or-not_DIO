# lab-natty-or-not_DIO

## Desafio de projeto da DIO [Natural ou Fake Natty? Como Vencer na Era das IAs Generativas!](https://web.dio.me/project/natural-ou-fake-natty-como-vencer-na-era-das-ias-generativas/learning/95e52735-b8ac-4657-bd4b-0a9cf3c1a5db?back=/track/coding-future-vivo-python-ai-backend-developer&tab=undefined&moduleId=undefined)

[Link para o repositÃ³rio da DIO](https://github.com/digitalinnovationone/lab-natty-or-not)

---

# Explorando IAs Generativas para Criar ConteÃºdo Criativo

## ğŸ“’ DescriÃ§Ã£o
Este projeto tem como objetivo explorar o potencial das InteligÃªncias Artificiais Generativas (IAGs) para criar conteÃºdos criativos e realistas, como imagens, textos, Ã¡udios, vÃ­deos e suas combinaÃ§Ãµes. Foram Utilizados diversas tÃ©cnicas e modelos de IAGs para gerar conteÃºdo inovador e surpreendente.

## ğŸ¤– Tecnologias Utilizadas
* Redes Neurais Recorrentes (RNNs)
  As RNNs sÃ£o projetadas para processar dados sequenciais, onde a ordem dos elementos na sequÃªncia Ã© importante, como texto, Ã¡udio e sÃ©ries temporais.
  Uma caracterÃ­stica chave das RNNs Ã© que elas mantÃªm estados ocultos que sÃ£o atualizados a cada passo de tempo, permitindo que a rede mantenha informaÃ§Ãµes de contexto sobre a sequÃªncia atÃ© o momento atual.
  No entanto, as RNNs tradicionais tÃªm dificuldade em capturar dependÃªncias de longo prazo em sequÃªncias devido ao problema de desvanecimento ou explosÃ£o do gradiente.  
  1. OpenAI GPT: Embora a versÃ£o completa do GPT (Generative Pre-trained Transformer) da OpenAI seja paga, eles oferecem uma versÃ£o menor chamada GPT-3.5 que pode ser usada gratuitamente para fins nÃ£o comerciais. O GPT-3.5 Ã© baseado em arquiteturas de RNNs e pode ser usado para gerar texto de forma criativa.
  2. NVIDIA Deep Learning Examples: A NVIDIA disponibiliza exemplos de aprendizado profundo em seu repositÃ³rio GitHub, que incluem modelos de RNNs prÃ©-treinados para vÃ¡rias tarefas. Esses modelos podem ser baixados e usados gratuitamente.
* Transformers
  Os Transformers sÃ£o uma arquitetura de rede neural baseada em mecanismos de atenÃ§Ã£o que foram introduzidos pela primeira vez no modelo "Attention is All You Need" em 2017.
  Os Transformers eliminam a necessidade de recorrÃªncia ao usar mecanismos de atenÃ§Ã£o para capturar as dependÃªncias entre elementos na sequÃªncia. Isso permite que eles capturem relaÃ§Ãµes de longo alcance de forma mais eficaz.
  Os modelos de Transformer consistem em mÃºltiplas camadas de auto-atendimento (self-attention) e camadas de feedforward, o que os torna altamente paralelizÃ¡veis e eficientes para treinamento em grandes conjuntos de dados.
  1. Hugging Face Inference API: A Hugging Face tambÃ©m oferece uma API de inferÃªncia que permite acessar modelos de Transformers prÃ©-treinados diretamente na nuvem, sem a necessidade de configurar um ambiente local. VocÃª pode usar essa API gratuitamente para fazer inferÃªncias com modelos de Transformers em tempo real.
* Outras tÃ©cnicas de IAs Generativas
  Autoregressive Models: sÃ£o modelos generativos que modelam a distribuiÃ§Ã£o conjunta de uma sequÃªncia de dados como o produto de distribuiÃ§Ãµes condicionais ao longo da sequÃªncia. Eles geram dados de forma iterativa, amostrando uma dimensÃ£o de cada vez, condicionada aos valores anteriores.
  1. PixelCNN: Uma arquitetura de modelo autoregressivo para geraÃ§Ã£o de imagens, implementada no TensorFlow e no PyTorch.
  2. WaveNet: Um modelo autoregressivo desenvolvido pelo DeepMind para geraÃ§Ã£o de Ã¡udio, implementado no TensorFlow.
